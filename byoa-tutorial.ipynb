{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BYOA Tutorial - Prophet Forecasting en Sagemaker\n",
    "La siguiente notebook muestra como integrar algoritmos propios a Amazon Sagemaker.\n",
    "Vamos a recorrer el camino para armar un pipeline de inferencia sobre el algoritmo Prophet para series de tiempo.\n",
    "El algoritmo se instala en un container de docker y luego nos sirve para hacer entrenamientos del modelo e inferencias en un endpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Armado del dataset\n",
    "Vamos a trabajar con un dataset publico que debemos bajar de Kaggle.\n",
    "Dicho dataset se denomina:\n",
    "_Avocado Prices: Historical data on avocado prices and sales volume in multiple US markets_\n",
    "y puede bajarse de: https://www.kaggle.com/neuromusic/avocado-prices/download\n",
    "Una vez bajado, debemos subirlo al mismo directorio donde estamos ejecutando esta notebook.\n",
    "El siguiente codigo prepara el dataset para que pueda entenderlo Prophet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nos quedamos solo con la fecha y las ventas\n",
    "df = pd.read_csv('avocado.csv')\n",
    "df = df[['Date', 'AveragePrice']].dropna()\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date')\n",
    "\n",
    "# Dejamos 1 solo registro por día con el promedio de ventas\n",
    "daily_df = df.resample('D').mean()\n",
    "d_df = daily_df.reset_index().dropna()\n",
    "\n",
    "# Formateamos los nombre de columnas como los espera Prophet\n",
    "d_df = d_df[['Date', 'AveragePrice']]\n",
    "d_df.columns = ['ds', 'y']\n",
    "d_df.head()\n",
    "\n",
    "# Guardamos el dataset resultante como avocado_daily.csv\n",
    "d_df.to_csv(\"avocado_daily.csv\",index = False , columns = ['ds', 'y'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2: Empaquetar y subir el algoritmo para usarlo con Amazon SageMaker\n",
    "\n",
    "### Una visión general de Docker\n",
    "\n",
    "Docker proporciona una forma simple de empaquetar código en un _image_ que es totalmente autónomo. Una vez que tenga una imagen, puede usar Docker para ejecutar un _container_ basado en esa imagen. Ejecutar un contenedor es igual que ejecutar un programa en la máquina, excepto que el contenedor crea un entorno totalmente autónomo para que el programa se ejecute. Los contenedores están aislados entre sí y del entorno host, por lo que la forma en que configura el programa es la forma en que se ejecuta, sin importar dónde lo ejecute.\n",
    "\n",
    "Docker es más poderoso que los administradores de entorno como conda o virtualenv porque (a) es completamente independiente del lenguaje y (b) comprende todo su entorno operativo, incluidos los comandos de inicio, las variables de entorno, etc.\n",
    "\n",
    "De alguna manera, un contenedor Docker es como una máquina virtual, pero es mucho más ligero. Por ejemplo, un programa que se ejecuta en un contenedor puede iniciarse en menos de un segundo y muchos contenedores pueden ejecutarse en la misma máquina física o instancia de máquina virtual.\n",
    "\n",
    "Docker utiliza un archivo simple llamado `Dockerfile` para especificar cómo se ensambla la imagen.\n",
    "Amazon SagMaker utiliza Docker para permitir a los usuarios entrenar e implementar algoritmos.\n",
    "\n",
    "En Amazon SageMaker, los contenedores Docker se invocan de cierta manera para el entrenamiento y de una forma ligeramente diferente para el hosting. En las siguientes secciones se describe cómo crear contenedores para el entorno de SageMaker.\n",
    "\n",
    "\n",
    "### Cómo Amazon SageMaker ejecuta el contenedor Docker\n",
    "\n",
    "Debido a que puede ejecutar la misma imagen en formación o hosting, Amazon SageMaker ejecuta el contenedor con el argumento `train` o `serve`. La forma en que su contenedor procesa este argumento depende del contenedor:\n",
    "\n",
    "* En el ejemplo aquí, no definimos un `ENTRYPOINT `en el Dockerfile para que Docker ejecute el comando `train` en tiempo de entrenamiento y `serve` en tiempo de servicio. En este ejemplo, los definimos como scripts ejecutables de Python, pero podrían ser cualquier programa que queramos iniciar en ese entorno.\n",
    "* Si especifica un programa como «ENTRYPOINT» en el archivo Dockerfile, ese programa se ejecutará al inicio y su primer argumento será `train` o `serve`. El programa puede entonces examinar ese argumento y decidir qué hacer.\n",
    "* Si está construyendo contenedores separados para entrenamiento y hosting (o construyendo solo para uno u otro), puede definir un programa como «ENTRYPOINT» en el archivo Dockerfile e ignorar (o verificar) el primer argumento pasado. \n",
    "\n",
    "#### Ejecutar el contenedor durante el entrenamiento\n",
    "\n",
    "Cuando Amazon SageMaker ejecuta el entrenamiento, el script `train` se ejecuta como un programa normal de Python. Una serie de archivos están dispuestos para su uso, bajo el directorio `/opt/ml`:\n",
    "\n",
    "    /opt/ml\n",
    "    ├── input\n",
    "    │   ├── config\n",
    "    │   │   ├── hyperparameters.json\n",
    "    │   │   └── resourceConfig.json\n",
    "    │   └── data\n",
    "    │       └── <channel_name>\n",
    "    │           └── <input data>\n",
    "    ├── model\n",
    "    │   └── <model files>\n",
    "    └── output\n",
    "        └── failure\n",
    "\n",
    "##### La entrada\n",
    "\n",
    "* `/opt/ml/input/config` contiene información para controlar cómo se ejecuta el programa. `hyperparameters.json` es un diccionario con formato JSON de nombres de hiperparámetros a valores. Estos valores siempre serán cadenas, por lo que es posible que deba convertirlos. `ResourceConfig.json` es un archivo con formato JSON que describe el diseño de red utilizado para la formación distribuida. Dado que scikit-learn no admite entrenamiento distribuido, lo ignoraremos aquí.\n",
    "* `/opt/ml/input/data/<channel_name>/` (para el modo Archivo) contiene los datos de entrada para ese canal. Los canales se crean en función de la llamada a CreateTrainingJob, pero generalmente es importante que los canales coincidan con lo que el algoritmo espera. Los archivos de cada canal se copiarán de S3 a este directorio, preservando la estructura de árbol indicada por la estructura de clave S3. \n",
    "* `/opt/ml/input/data/<channel_name>_<epoch_number>`(para el modo Pipe) es la tubería para una época determinada. Las épocas comienzan en cero y suben por uno cada vez que las lees. No hay límite en el número de épocas que puede ejecutar, pero debe cerrar cada tubería antes de leer la siguiente época.\n",
    "    \n",
    "##### La salida\n",
    "\n",
    "* `/opt/ml/model/` es el directorio donde se escribe el modelo que genera su algoritmo. Su modelo puede estar en cualquier formato que desee. Puede ser un solo archivo o un árbol de directorios completo. SagMaker empaquetará cualquier archivo de este directorio en un archivo comprimido tar. Este archivo estará disponible en la ubicación S3 devuelta en el resultado `DescribeTrainingJob`.\n",
    "* `/opt/ml/output` es un directorio donde el algoritmo puede escribir un archivo `failure` que describe por qué el trabajo falló. El contenido de este archivo se devolverá en el campo `FailureReason` del resultado `DescribeTrainingJob`. Para los trabajos que tienen éxito, no hay razón para escribir este archivo, ya que se ignorará.\n",
    "\n",
    "#### Ejecutando el contenedor durante el hosting\n",
    "\n",
    "El hosting tiene un modelo muy diferente al de entrenamiento porque debe responder a las solicitudes de inferencia que llegan a través de HTTP. En este ejemplo, utilizamos codigo escrito en Python recomendado para proporcionar un servicio robusto y escalable de solicitudes de inferencia:\n",
    "\n",
    "Amazon SagMaker utiliza dos URL en el contenedor:\n",
    "\n",
    "* `/ping` recibirá solicitudes `GET` de la infraestructura. Devuelve 200 si el contenedor está abierto y acepta solicitudes.\n",
    "* `/invocations` es el punto final que recibe solicitudes `POST` de inferencia del cliente. El formato de la solicitud y la respuesta depende del algoritmo. Si el cliente suministró los encabezados `ContentType` y `Accept`, éstos también se pasarán. \n",
    "\n",
    "El contenedor tendrá los archivos de modelo en el mismo lugar en el que se escribieron durante el entrenamiento:\n",
    "\n",
    "    /opt/ml\n",
    "    └── model\n",
    "        └── <model files>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partes del Container\n",
    "\n",
    "En el directorio `container` están todos los componentes que necesita para empaquetar el algoritmo de muestra para Amazon SageManager:\n",
    "\n",
    "    .\n",
    "    ├── Dockerfile\n",
    "    ├── build_and_push.sh\n",
    "    └── decision_trees\n",
    "        ├── nginx.conf\n",
    "        ├── predictor.py\n",
    "        ├── serve\n",
    "        ├── train\n",
    "        └── wsgi.py\n",
    "\n",
    "\n",
    "Vamos a ver cada uno:\n",
    "\n",
    "* __`Dockerfile`__ describe cómo construir la imagen de contenedor Docker. Más detalles a continuación.\n",
    "* __`build_and_push.sh`__ es un script que utiliza Dockerfile para construir sus imágenes de contenedor y luego lo publica (push) a ECR. Invocaremos los comandos directamente más adelante en este notebook, pero se puede copiar y ejecutar el script para otros algoritmos.\n",
    "* __`prophet`__ es el directorio que contiene los archivos que se instalarán en el contenedor.\n",
    "* __`local_test`__ es un directorio que muestra cómo probar el nuevo contenedor en cualquier equipo que pueda ejecutar Docker, incluida una Notebook Instance de Amazon SageMaker. Con este método, puede iterar rápidamente utilizando pequeños conjuntos de datos para eliminar cualquier error estructural antes de utilizar el contenedor con Amazon SageMaker. \n",
    "\n",
    "Los archivos que vamos a poner en el contenedor son:\n",
    "\n",
    "* __`nginx.conf`__ es el archivo de configuración para el front-end nginx. Generalmente, debería poder tomar este archivo tal como está.\n",
    "* __`predictor.py`__ es el programa que realmente implementa el servidor web Flask y las predicciones de Prophet para esta aplicación. \n",
    "* __`serve`__ es el programa iniciado cuando se inicia el contenedor para hosting. Simplemente lanza el servidor gunicorn que ejecuta múltiples instancias de la aplicación Flask definida en `predictor.py`. Debería poder tomar este archivo tal como está.\n",
    "* __`train`__ es el programa que se invoca cuando se ejecuta el contenedor para el entrenamiento. \n",
    "* __`wsgi.py`__ es un pequeño envoltorio utilizado para invocar la aplicación Flask. Debería poder tomar este archivo tal como está.\n",
    "\n",
    "En resumen, los dos archivos con codigo especifico de Prophet son `train` y `predictor.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El archivo Dockerfile\n",
    "\n",
    "El archivo Dockerfile describe la imagen que queremos crear. Es una descripción de la instalación completa del sistema operativo del sistema que desea ejecutar. Un contenedor Docker que se ejecuta es bastante más ligero que un sistema operativo completo, sin embargo, porque aprovecha Linux en la máquina host para las operaciones básicas. \n",
    "\n",
    "Para este ejemplo, comenzaremos desde una instalación estándar de Ubuntu y ejecutaremos las herramientas normales para instalar las cosas que necesita Prophet. Finalmente, agregamos el código que implementa Prophet al contenedor y configuramos el entorno correcto para que se ejecute correctamente.\n",
    "\n",
    "El siguiente es el Dockerfile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Build an image that can do training and inference in SageMaker\n",
      "# This is a Python 3 image that uses the nginx, gunicorn, flask stack\n",
      "# for serving inferences in a stable way.\n",
      "\n",
      "FROM ubuntu:16.04\n",
      "\n",
      "MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      "\n",
      "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
      "         wget \\\n",
      "         curl \\\n",
      "         python-dev \\\n",
      "         build-essential libssl-dev libffi-dev \\\n",
      "         libxml2-dev libxslt1-dev zlib1g-dev \\\n",
      "         nginx \\\n",
      "         ca-certificates \\\n",
      "    && rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "RUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py && \\\n",
      "    python get-pip.py && \\\n",
      "    rm get-pip.py\n",
      " \n",
      "RUN pip --no-cache-dir install \\\n",
      "        numpy \\\n",
      "        scipy \\\n",
      "        sklearn \\\n",
      "        pandas \\\n",
      "        flask \\\n",
      "        gevent \\\n",
      "        gunicorn \\\n",
      "        pystan \n",
      "\n",
      "RUN pip --no-cache-dir install \\\n",
      "        fbprophet \n",
      "        \n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Set up the program in the image\n",
      "COPY prophet /opt/program\n",
      "WORKDIR /opt/program\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El archivo train\n",
    "\n",
    "El archivo train describe la forma en la que vamos a realizar el entrenamiento.\n",
    "El archivo Prophet-Docker/container/prophet/train contiene el codigo especifico de entrenamiento para Prophet.\n",
    "Debemos modificar la funcion train() de la siguiente manera:\n",
    "\n",
    "    def train():\n",
    "        print('Starting the training.')\n",
    "        try:\n",
    "            # Read in any hyperparameters that the user passed with the training job\n",
    "            with open(param_path, 'r') as tc:\n",
    "                trainingParams = json.load(tc)\n",
    "            # Take the set of files and read them all into a single pandas dataframe\n",
    "            input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]\n",
    "            if len(input_files) == 0:\n",
    "                raise ValueError(('There are no files in {}.\\n' +\n",
    "                                  'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                                  'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                                  'does not have permission to access the data.').format(training_path, channel_name))\n",
    "            raw_data = [ pd.read_csv(file, error_bad_lines=False ) for file in input_files ]\n",
    "            train_data = pd.concat(raw_data)\n",
    "            train_data.columns = ['ds', 'y']\n",
    "\n",
    "            # Usamos Prophet para entrenar el modelo.\n",
    "            clf = Prophet()\n",
    "            clf = clf.fit(train_data)\n",
    "\n",
    "            # save the model\n",
    "            with open(os.path.join(model_path, 'prophet-model.pkl'), 'w') as out:\n",
    "                pickle.dump(clf, out)\n",
    "            print('Training complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El archivo predictor.py\n",
    "\n",
    "El archivo predictor.py describe la forma en la que vamos a realizar las predicciones.\n",
    "El archivo Prophet-Docker/container/prophet/predictor.py contiene el codigo especifico de prediccion para Prophet.\n",
    "Debemos modificar la funcion predict() de la siguiente manera:\n",
    "\n",
    "    def predict(cls, input):\n",
    "        \"\"\"For the input, do the predictions and return them.\n",
    "\n",
    "        Args:\n",
    "            input (a pandas dataframe): The data on which to do the predictions. There will be\n",
    "                one prediction per row in the dataframe\"\"\"\n",
    "        clf = cls.get_model()\n",
    "        future = clf.make_future_dataframe(periods=int(input.iloc[0]))\n",
    "        print(int(input.iloc[0]))\n",
    "        print(input)\n",
    "        forecast = clf.predict(future)\n",
    "              \n",
    "        return forecast.tail(int(input.iloc[0]))\n",
    "\n",
    "\n",
    "Y luego la funcion transformation() de la siguiente manera: \n",
    "\n",
    "    def transformation():\n",
    "        \"\"\"Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\n",
    "        it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\n",
    "        just means one prediction per line, since there's a single column.\n",
    "        \"\"\"\n",
    "        data = None\n",
    "\n",
    "        # Convert from CSV to pandas\n",
    "        if flask.request.content_type == 'text/csv':\n",
    "            data = flask.request.data.decode('utf-8')\n",
    "            s = StringIO.StringIO(data)\n",
    "            data = pd.read_csv(s, header=None)\n",
    "        else:\n",
    "            return flask.Response(response='This predictor only supports CSV data', status=415, mimetype='text/plain')\n",
    "\n",
    "        print('Invoked with {} records'.format(data.shape[0]))\n",
    "\n",
    "        # Do the prediction\n",
    "        predictions = ScoringService.predict(data)\n",
    "\n",
    "        # Convert from numpy back to CSV\n",
    "        out = StringIO.StringIO()\n",
    "        pd.DataFrame({'results':[predictions]}, index=[0]).to_csv(out, header=False, index=False)\n",
    "        result = out.getvalue()\n",
    "\n",
    "        return flask.Response(response=result, status=200, mimetype='text/csv')\n",
    " \n",
    "\n",
    "Basicamente modificamos la linea:\n",
    "\n",
    "        pd.DataFrame({'results':predictions}).to_csv(out, header=False, index=False)\n",
    " \n",
    "Por la linea:\n",
    "\n",
    "        pd.DataFrame({'results':[predictions]}, index=[0]).to_csv(out, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Uso de Prophet en Amazon SageMaker\n",
    "Ahora que tenemos todos los archivos creados, vamos a utilizar Prophet en Sagemaker\n",
    "\n",
    "## Armado del Container\n",
    "Empezamos construyendo y registrando el container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  63.49kB\n",
      "Step 1/11 : FROM ubuntu:16.04\n",
      " ---> c6a43cd4801e\n",
      "Step 2/11 : MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      " ---> Using cache\n",
      " ---> c0ea7ed783e7\n",
      "Step 3/11 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          curl          python-dev          build-essential libssl-dev libffi-dev          libxml2-dev libxslt1-dev zlib1g-dev          nginx          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 17bd5ae1900b\n",
      "Step 4/11 : RUN curl -fSsL -O https://bootstrap.pypa.io/get-pip.py &&     python get-pip.py &&     rm get-pip.py\n",
      " ---> Using cache\n",
      " ---> e1f1939e31e1\n",
      "Step 5/11 : RUN pip --no-cache-dir install         numpy         scipy         sklearn         pandas         flask         gevent         gunicorn         pystan\n",
      " ---> Using cache\n",
      " ---> 8ff73a969fc2\n",
      "Step 6/11 : RUN pip --no-cache-dir install         fbprophet\n",
      " ---> Using cache\n",
      " ---> 815dc3862860\n",
      "Step 7/11 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 35c7a5aac761\n",
      "Step 8/11 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> ef336e62f7f5\n",
      "Step 9/11 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 290851f5e67b\n",
      "Step 10/11 : COPY prophet /opt/program\n",
      " ---> Using cache\n",
      " ---> 4b6b9e299087\n",
      "Step 11/11 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> fe21109f123a\n",
      "Successfully built fe21109f123a\n",
      "Successfully tagged sagemaker-prophet:latest\n",
      "The push refers to repository [563487891580.dkr.ecr.us-east-1.amazonaws.com/sagemaker-prophet]\n",
      "00236715c809: Preparing\n",
      "15d05e86afe8: Preparing\n",
      "168ec802cd02: Preparing\n",
      "e5e30443c428: Preparing\n",
      "022ecae92fcb: Preparing\n",
      "77008e118980: Preparing\n",
      "6cb741cb00b7: Preparing\n",
      "f36b28e4310d: Preparing\n",
      "91d23cf5425a: Preparing\n",
      "f36b28e4310d: Waiting\n",
      "91d23cf5425a: Waiting\n",
      "77008e118980: Waiting\n",
      "6cb741cb00b7: Waiting\n",
      "e5e30443c428: Layer already exists\n",
      "022ecae92fcb: Layer already exists\n",
      "168ec802cd02: Layer already exists\n",
      "15d05e86afe8: Layer already exists\n",
      "00236715c809: Layer already exists\n",
      "77008e118980: Layer already exists\n",
      "6cb741cb00b7: Layer already exists\n",
      "91d23cf5425a: Layer already exists\n",
      "f36b28e4310d: Layer already exists\n",
      "latest: digest: sha256:92c9042b85d712100a6b73c3ab8257944a85f1e46a8d69f78b1dbbb72427f031 size: 2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.27 ms, sys: 594 µs, total: 9.87 ms\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-prophet\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x prophet/train\n",
    "chmod +x prophet/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado del Entorno de Entrenamiento\n",
    "Inicializamos la sesion, rol de ejecucion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 408 ms, sys: 40.3 ms, total: 448 ms\n",
      "Wall time: 503 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "prefix = 'DEMO-prophet-byo'\n",
    "role = get_execution_role()\n",
    "sess = sage.Session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subimos los datos a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = 'data'\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos el modelo\n",
    "Utilizando los datos subidos a S3, entrenamos el modelo levantando una instancia ml.c4.2xlarge. \n",
    "Sagemaker va a dejar el modelo entrenado en el directorio /output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-27 16:00:08 Starting - Starting the training job...\n",
      "2019-12-27 16:00:09 Starting - Launching requested ML instances......\n",
      "2019-12-27 16:01:13 Starting - Preparing the instances for training...\n",
      "2019-12-27 16:01:58 Downloading - Downloading input data\n",
      "2019-12-27 16:01:58 Training - Downloading the training image...\n",
      "2019-12-27 16:02:34 Training - Training image download completed. Training in progress..\u001b[34mINFO:matplotlib.font_manager:font search path ['/usr/local/lib/python2.7/dist-packages/matplotlib/mpl-data/fonts/ttf', '/usr/local/lib/python2.7/dist-packages/matplotlib/mpl-data/fonts/afm', '/usr/local/lib/python2.7/dist-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\u001b[0m\n",
      "\u001b[34mINFO:matplotlib.font_manager:generated new fontManager\u001b[0m\n",
      "\u001b[34mERROR:fbprophet:Importing matplotlib failed. Plotting will not work.\u001b[0m\n",
      "\u001b[34mERROR:fbprophet:Importing plotly failed. Interactive plots will not work.\u001b[0m\n",
      "\u001b[34mStarting the training.\u001b[0m\n",
      "\u001b[34mINFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\u001b[0m\n",
      "\u001b[34mINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\u001b[0m\n",
      "\u001b[34mInitial log joint probability = -2.69053\n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "      99       450.532    0.00314344       85.9671       2.069      0.2069      135   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     155       451.709   0.000185872       84.4313     3.9e-06       0.001      245  LS failed, Hessian reset \n",
      "     199       451.872   8.44141e-05       56.0574      0.5636           1      296   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     299       452.598     0.0313561       102.286      0.9683           1      420   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     300       452.617   0.000446882       108.925   4.369e-06       0.001      499  LS failed, Hessian reset \n",
      "     386        453.08   1.71918e-05       65.6637   2.482e-07       0.001      648  LS failed, Hessian reset \n",
      "     399       453.081   6.92601e-07       51.7009       1.002      0.2039      667   \n",
      "    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n",
      "     408       453.081   3.47948e-09       59.4053     0.01718           1      682   \u001b[0m\n",
      "\u001b[34mOptimization terminated normally: \n",
      "  Convergence detected: absolute parameter change was below tolerance\u001b[0m\n",
      "\u001b[34mTraining complete.\u001b[0m\n",
      "\n",
      "2019-12-27 16:02:45 Uploading - Uploading generated training model\n",
      "2019-12-27 16:02:45 Completed - Training job completed\n",
      "Training seconds: 60\n",
      "Billable seconds: 60\n",
      "CPU times: user 362 ms, sys: 18 ms, total: 380 ms\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-prophet:latest'.format(account, region)\n",
    "\n",
    "tseries = sage.estimator.Estimator(image,\n",
    "                       role, \n",
    "                        1, \n",
    "                        'ml.c4.2xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "tseries.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armado de endpoint para inferencia\n",
    "Utilizando el modelo recien entrenado, creamos un endpoint para inferencia hosteado en una instancia ml.c4.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------!CPU times: user 518 ms, sys: 39.5 ms, total: 557 ms\n",
      "Wall time: 9min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = tseries.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de inferencia\n",
    "Finalmente le pedimos al modelo que nos prediga las ventas de los proximos 30 dias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"            ds     trend  trend_lower  ...  yearly_lower  yearly_upper      yhat\\n169 2018-03-26  1.473312     1.473312  ...     -0.076117     -0.076117  1.397195\\n170 2018-03-27  1.472971     1.472971  ...     -0.072531     -0.072531  1.400440\\n171 2018-03-28  1.472631     1.472631  ...     -0.068829     -0.068829  1.403802\\n172 2018-03-29  1.472291     1.472291  ...     -0.065070     -0.065070  1.407221\\n173 2018-03-30  1.471950     1.471950  ...     -0.061313     -0.061313  1.410637\\n174 2018-03-31  1.471610     1.471610  ...     -0.057619     -0.057619  1.413991\\n175 2018-04-01  1.471270     1.471270  ...     -0.054048     -0.054048  1.417222\\n176 2018-04-02  1.470929     1.470929  ...     -0.050657     -0.050657  1.420273\\n177 2018-04-03  1.470589     1.470589  ...     -0.047500     -0.047500  1.423089\\n178 2018-04-04  1.470248     1.470241  ...     -0.044627     -0.044627  1.425622\\n179 2018-04-05  1.469908     1.469861  ...     -0.042080     -0.042080  1.427828\\n180 2018-04-06  1.469568     1.469467  ...     -0.039896     -0.039896  1.429672\\n181 2018-04-07  1.469227     1.469070  ...     -0.038104     -0.038104  1.431123\\n182 2018-04-08  1.468887     1.468698  ...     -0.036725     -0.036725  1.432162\\n183 2018-04-09  1.468547     1.468307  ...     -0.035770     -0.035770  1.432776\\n184 2018-04-10  1.468206     1.467909  ...     -0.035245     -0.035245  1.432961\\n185 2018-04-11  1.467866     1.467514  ...     -0.035144     -0.035144  1.432722\\n186 2018-04-12  1.467526     1.467111  ...     -0.035454     -0.035454  1.432072\\n187 2018-04-13  1.467185     1.466734  ...     -0.036155     -0.036155  1.431030\\n188 2018-04-14  1.466845     1.466344  ...     -0.037221     -0.037221  1.429624\\n189 2018-04-15  1.466505     1.465922  ...     -0.038616     -0.038616  1.427889\\n190 2018-04-16  1.466164     1.465522  ...     -0.040301     -0.040301  1.425863\\n191 2018-04-17  1.465824     1.465120  ...     -0.042234     -0.042234  1.423590\\n192 2018-04-18  1.465483     1.464696  ...     -0.044367     -0.044367  1.421117\\n193 2018-04-19  1.465143     1.464293  ...     -0.046650     -0.046650  1.418493\\n194 2018-04-20  1.464803     1.463839  ...     -0.049034     -0.049034  1.415769\\n195 2018-04-21  1.464462     1.463360  ...     -0.051467     -0.051467  1.412995\\n196 2018-04-22  1.464122     1.462924  ...     -0.053900     -0.053900  1.410222\\n197 2018-04-23  1.463782     1.462459  ...     -0.056284     -0.056284  1.407497\\n198 2018-04-24  1.463441     1.461976  ...     -0.058574     -0.058574  1.404867\\n\\n[30 rows x 16 columns]\"\\n'\n",
      "CPU times: user 11.5 ms, sys: 0 ns, total: 11.5 ms\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p = predictor.predict(\"30\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
